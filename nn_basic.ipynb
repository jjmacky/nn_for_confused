{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# A basic feed forward neural network\n",
    "# Neural networks for people who get confused easily\n",
    "# James McCammon\n",
    "# May 2024\n",
    "###################################################\n",
    "\n",
    "###\n",
    "# Define classes for our neural network\n",
    "###\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from keras.datasets import mnist\n",
    "\n",
    "class NNManager:\n",
    "    def __init__(self, n_features, layers):\n",
    "        self.network = []\n",
    "        p_l = n_features\n",
    "        for n_nodes in layers:   \n",
    "            self.network.append(Dense_layer(p_l, n_nodes))\n",
    "            p_l = n_nodes\n",
    "\n",
    "    def batch_manager(self, inputs, true_labels, learning_rate, batch_size):\n",
    "        batched_inputs = self.batch_splitter(inputs, batch_size)\n",
    "        batched_true_labels = self.batch_splitter(true_labels, batch_size)\n",
    "\n",
    "        for batch_number in range(len(batched_inputs)):\n",
    "            self.optimizer(batched_inputs[batch_number], batched_true_labels[batch_number], learning_rate)\n",
    "\n",
    "    def batch_splitter(self, inputs, batch_size):\n",
    "        batches = [inputs[k:k + batch_size] for k in range(0, len(inputs), batch_size)]\n",
    "        if(len(batches[-1]) != batch_size):\n",
    "            batches.pop(-1)\n",
    "        return np.array(batches)\n",
    "\n",
    "    def optimizer(self, inputs, true_labels, learning_rate):\n",
    "        predicted_probs = self.nn_forward_pass(inputs)\n",
    "        self.calculate_loss(predicted_probs, true_labels)\n",
    "        self.nn_backward_pass(inputs, predicted_probs, true_labels, learning_rate)\n",
    "    \n",
    "    def nn_forward_pass(self, inputs):\n",
    "        previous_layer_output = inputs\n",
    "\n",
    "        #Iterate over each layer in the network\n",
    "        for layer in self.network:\n",
    "            layer.forward(previous_layer_output)\n",
    "\n",
    "            if (layer == self.network[-1]): # Check if layer is last layer\n",
    "                layer.post_activation_output = Activation.softmax(layer.pre_activation_output)\n",
    "                predicted_probs = layer.post_activation_output \n",
    "            else:\n",
    "                layer.post_activation_output = Activation.relu(layer.pre_activation_output)            \n",
    "            \n",
    "            previous_layer_output = layer.post_activation_output\n",
    "\n",
    "        return predicted_probs\n",
    "    \n",
    "    def calculate_loss(self, predicted_probs, true_labels):\n",
    "        loss = Loss.cross_entropy(true_labels, predicted_probs)\n",
    "        return loss\n",
    "            \n",
    "    def nn_backward_pass(self, inputs, predicted_probs, true_labels, learning_rate, clip_value = 1.0):\n",
    "        cumulative_derivative = predicted_probs - true_labels\n",
    "        backward_network = np.flip(self.network)\n",
    "\n",
    "        for idx, layer in enumerate(backward_network):\n",
    "            if idx == len(backward_network) - 1:\n",
    "                current_layer_post_activation = inputs.T\n",
    "                \n",
    "            else:\n",
    "                previous_layer_pre_activation = backward_network[idx + 1].pre_activation_output\n",
    "                current_layer_post_activation = backward_network[idx + 1].post_activation_output.T\n",
    "            \n",
    "            weight_gradient = current_layer_post_activation @ cumulative_derivative\n",
    "            bias_gradient = np.sum(cumulative_derivative, axis=0, keepdims=True)\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            weight_gradient = np.clip(weight_gradient, -clip_value, clip_value)\n",
    "            bias_gradient = np.clip(bias_gradient, -clip_value, clip_value)\n",
    "\n",
    "            layer.weights -= learning_rate * weight_gradient.T\n",
    "            layer.biases -= learning_rate * bias_gradient.T\n",
    "            \n",
    "            if idx != len(backward_network) - 1:\n",
    "                cumulative_derivative = (cumulative_derivative @ layer.weights) * (previous_layer_pre_activation > 0)\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    @staticmethod\n",
    "    def predict(NNManager, input_to_predict, true_label):\n",
    "        prediction = np.argmax(NNManager.nn_forward_pass(input_to_predict))\n",
    "        print(f\"Class prediction is {prediction}\")\n",
    "        print(f\"Actual class is {np.argmax(true_label)}\")\n",
    "        return prediction\n",
    "\n",
    "class Dense_layer:\n",
    "    def __init__(self, n_inputs, n_nodes):\n",
    "        self.name = ''.join(random.choices(string.ascii_uppercase, k=5))\n",
    "        self.n_nodes = n_nodes\n",
    "        self.weights = self.initialize_weights(n_nodes, n_inputs)\n",
    "        self.biases = self.initialize_biases(n_nodes)\n",
    "        self.pre_activation_output = None\n",
    "        self.post_activation_output = None\n",
    "\n",
    "    # Use the so-called \"He initalization\"\n",
    "    def initialize_weights(self, n_nodes, n_inputs):\n",
    "        mu = 0\n",
    "        sigma = np.sqrt(2 / n_inputs)\n",
    "        return np.random.normal(mu, sigma, (n_nodes, n_inputs))\n",
    "    \n",
    "    def initialize_biases(self, n_nodes):\n",
    "        return np.zeros((n_nodes, 1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.pre_activation_output = input @ self.weights.T + self.biases.T # Transpose biases so broadcasting works as expected\n",
    "        return self.pre_activation_output\n",
    "    \n",
    "class Activation:\n",
    "    @staticmethod\n",
    "    def relu(layer_input):\n",
    "        post_activation_output = np.maximum(0, layer_input)\n",
    "        return post_activation_output\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        predicted_probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "        return predicted_probs\n",
    "\n",
    "class Loss:\n",
    "    @staticmethod\n",
    "    def cross_entropy(true_labels, predicted_probs):\n",
    "        batch_size = true_labels.shape[0]\n",
    "        predicted_probs_clipped = np.clip(predicted_probs, 1e-7, 1 - 1e-7)\n",
    "        log_probs = np.log(predicted_probs_clipped)\n",
    "        loss = -1 * np.multiply(true_labels, log_probs).sum() / batch_size\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Test Loss: 1.7810\n",
      "Test Accuracy: 0.8638\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Uses the classes on the well-known MNIST dataset\n",
    "###\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "train_images = train_images.reshape(train_images.shape[0], -1)\n",
    "test_images = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_labels = one_hot_encode(train_labels, 10)\n",
    "test_labels = one_hot_encode(test_labels, 10)\n",
    "\n",
    "# Define the neural network parameters\n",
    "input_size = 784  # 28x28\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Initialize and train the neural network\n",
    "nn_manager = NNManager(input_size, [hidden_size, output_size])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    permutation = np.random.permutation(train_images.shape[0])\n",
    "    train_images_shuffled = train_images[permutation]\n",
    "    train_labels_shuffled = train_labels[permutation]\n",
    "    \n",
    "    nn_manager.batch_manager(train_images_shuffled, train_labels_shuffled, learning_rate, batch_size)\n",
    "\n",
    "# Evaluate the neural network on test data\n",
    "predicted_probs = nn_manager.nn_forward_pass(test_images)\n",
    "test_loss = nn_manager.calculate_loss(predicted_probs, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = np.argmax(predicted_probs, axis=1)\n",
    "labels = np.argmax(test_labels, axis=1)\n",
    "accuracy = np.mean(predictions == labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prediction is 8\n",
      "Actual class is 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally we can predict the class of a specific image and compare it against the actual class label\n",
    "image_number = 403\n",
    "image_to_predict = test_images[image_number]\n",
    "true_label = test_labels[image_number]\n",
    "Predictor.predict(nn_manager, image_to_predict, true_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
